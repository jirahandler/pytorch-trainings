{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install and import dependencies\n",
    "# conda create -n kf_py312 python=3.12\n",
    "# conda activate kf_py312\n",
    "# pip install ipykernel uproot h5py numpy pandas tensorflow scikit-learn\n",
    "## In Ubuntu do:\n",
    "# sudo apt update\n",
    "# sudo apt --fix-broken install\n",
    "# sudo apt install \\\n",
    "#  libsqlite3-0=3.37.2-2ubuntu0.4 \\\n",
    "#  libsqlite3-dev=3.37.2-2ubuntu0.4 \\\n",
    "#  sqlite3=3.37.2-2ubuntu0.4\n",
    "# sudo apt-mark hold libsqlite3-0 libsqlite3-dev sqlite3\n",
    "# # OR with conda (not required)\n",
    "# conda install -c conda-forge openssl sqlite\n",
    "# conda install -c conda-forge cudatoolkit-dev cudnn tensorflow ipykernel uproot\\\n",
    "# h5py pandas scikit-learn matplotlib awkward numpy\n",
    "\n",
    "import sys, os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Python\", sys.version.split()[0])\n",
    "print(\"NumPy \", np.__version__)\n",
    "print(\"Pandas\", pd.__version__)\n",
    "print(\"Awkward\", ak.__version__)\n",
    "print(\"TensorFlow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cell 2: Define Data Loader\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "\n",
    "# ───────────────────────── 1. Helper functions ──────────────────────────\n",
    "def load_h5_data(files, label):\n",
    "    dfs, ys, ws = [], [], []\n",
    "    for f in files:\n",
    "        with h5py.File(f, \"r\") as hf:\n",
    "            c0 = [n.decode() for n in hf[\"df/block0_items\"][:]]\n",
    "            v0 = pd.DataFrame(hf[\"df/block0_values\"][:], columns=c0)\n",
    "            c1 = [n.decode() for n in hf[\"df/block1_items\"][:]]\n",
    "            v1 = pd.DataFrame(hf[\"df/block1_values\"][:], columns=c1)\n",
    "        df = pd.concat([v0, v1], axis=1)\n",
    "        dfs.append(df[selected_variables])\n",
    "        ws.append(df[\"weight\"].values if \"weight\" in df.columns else np.ones(len(df)))\n",
    "        ys.append(np.full(len(df), label))\n",
    "    return (\n",
    "        pd.concat(dfs, axis=0).reset_index(drop=True),\n",
    "        np.concatenate(ys),\n",
    "        np.concatenate(ws),\n",
    "    )\n",
    "\n",
    "def load_root_data(files, label, tree=\"sel_tree\"):\n",
    "    dfs, ys, ws = [], [], []\n",
    "    for f in files:\n",
    "        with uproot.open(f) as rf:\n",
    "            if tree not in rf: continue\n",
    "            arr = rf[tree].arrays(selected_variables + [\"weight\"], library=\"np\")\n",
    "        dfs.append(pd.DataFrame({v: arr[v] for v in selected_variables}))\n",
    "        ws.append(arr[\"weight\"]) # Preserving original weight\n",
    "        ys.append(np.full(arr[selected_variables[0]].shape, label))\n",
    "    return (\n",
    "        pd.concat(dfs, axis=0).reset_index(drop=True),\n",
    "        np.concatenate(ys),\n",
    "        np.concatenate(ws),\n",
    "    )\n",
    "\n",
    "# ───────────────────────── 2. Configuration ─────────────────────────────\n",
    "selected_variables = ['jet1_pt','jet1_eta','jet1met_dphi','met_sig','met_pt']\n",
    "\n",
    "base_dir_LQ         = \"/home/sgoswami/monobcntuples/\"\n",
    "signal_masses_LQ    = [\"500\",\"1000\",\"1400\",\"2000\",\"2500\",\"2800\"]\n",
    "base_dir_stop       = \"/home/sgoswami/monobcntuples/run3_btag/all\"\n",
    "signal_pattern_stop = os.path.join(base_dir_stop, \"singlestop\", \"basicSel_sT_*_*.root\")\n",
    "\n",
    "bkg_procs = [\"ttbar\",\"singletop\",\"dijet\",\"diboson\",\"wlnu\",\"zll\",\"znunu\"]\n",
    "\n",
    "# ───────────────────────── 3. Discover files ────────────────────────────\n",
    "def filter_ok(paths):\n",
    "    return [f for f in paths if \"_histogram\" not in f and \"_cutflow\" not in f]\n",
    "\n",
    "sig_files_LQ = []\n",
    "for m in signal_masses_LQ:\n",
    "    sig_files_LQ += filter_ok(glob.glob(os.path.join(base_dir_LQ, f\"mass_{m}\", \"basicSel_mass_*.h5\")))\n",
    "sig_files_stop = filter_ok(glob.glob(signal_pattern_stop))\n",
    "\n",
    "bkg_files_LQ_flat, bkg_files_stop_flat = [], []\n",
    "for p in bkg_procs:\n",
    "    bkg_files_LQ_flat.extend(filter_ok(glob.glob(os.path.join(base_dir_LQ, f\"{p}_mc20e\", \"*.h5\"))))\n",
    "    root_path = os.path.join(base_dir_stop, p, f\"basicSel_{p}.root\")\n",
    "    if os.path.exists(root_path):\n",
    "        bkg_files_stop_flat.append(root_path)\n",
    "\n",
    "print(\">>> DEBUG: File Discovery Complete.\")\n",
    "print(f\"    Found {len(sig_files_LQ)} LQ signal files and {len(bkg_files_LQ_flat)} LQ background files.\")\n",
    "print(f\"    Found {len(sig_files_stop)} Stop signal files and {len(bkg_files_stop_flat)} Stop background files.\")\n",
    "\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "# NEW LOADING AND BALANCING STRATEGY\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "# ────────────────── 4. Process LQ Dataset (Signal + Background) ───────────────\n",
    "print(\"\\n--- Processing LQ Dataset ---\")\n",
    "# 4.1: Load LQ signal\n",
    "X_LQ_sig, y_LQ_sig, w_LQ_sig = load_h5_data(sig_files_LQ, 1)\n",
    "N_sig_LQ = len(X_LQ_sig)\n",
    "print(f\"Loaded {N_sig_LQ} LQ signal events. Target for background is {N_sig_LQ}.\")\n",
    "\n",
    "# 4.2: Load all LQ background file-by-file to get their sizes\n",
    "lq_bkg_dfs, lq_bkg_sizes, lq_bkg_weights = [], [], []\n",
    "for f in bkg_files_LQ_flat:\n",
    "    Xf, _, wf = load_h5_data([f], 0)\n",
    "    lq_bkg_dfs.append(Xf)\n",
    "    lq_bkg_sizes.append(len(Xf))\n",
    "    lq_bkg_weights.append(wf)\n",
    "total_lq_bkg_events = sum(lq_bkg_sizes)\n",
    "print(f\"Found {total_lq_bkg_events} total available LQ background events across {len(lq_bkg_dfs)} files.\")\n",
    "\n",
    "# 4.3: Determine how many events to draw from each LQ background file proportionally\n",
    "target_per_file_lq = [int(round(N_sig_LQ * (sz / total_lq_bkg_events))) for sz in lq_bkg_sizes]\n",
    "while sum(target_per_file_lq) < N_sig_LQ: target_per_file_lq[np.argmax(lq_bkg_sizes)] += 1\n",
    "while sum(target_per_file_lq) > N_sig_LQ: target_per_file_lq[np.argmax(target_per_file_lq)] -= 1\n",
    "\n",
    "# 4.4: Sample from each LQ background file and shuffle the result\n",
    "X_LQ_bkg_parts, y_LQ_bkg_parts, w_LQ_bkg_parts = [], [], []\n",
    "for df, wt, k in zip(lq_bkg_dfs, lq_bkg_weights, target_per_file_lq):\n",
    "    if k == 0: continue\n",
    "    sel = np.random.choice(len(df), k, replace=False)\n",
    "    X_LQ_bkg_parts.append(df.iloc[sel])\n",
    "    y_LQ_bkg_parts.append(np.zeros(k, dtype=np.int64))\n",
    "    w_LQ_bkg_parts.append(wt[sel])\n",
    "\n",
    "X_LQ_bkg = pd.concat(X_LQ_bkg_parts, axis=0)\n",
    "y_LQ_bkg = np.concatenate(y_LQ_bkg_parts)\n",
    "w_LQ_bkg = np.concatenate(w_LQ_bkg_parts)\n",
    "\n",
    "lq_bkg_perm = np.random.permutation(len(X_LQ_bkg))\n",
    "X_LQ_bkg, y_LQ_bkg, w_LQ_bkg = X_LQ_bkg.iloc[lq_bkg_perm], y_LQ_bkg[lq_bkg_perm], w_LQ_bkg[lq_bkg_perm]\n",
    "print(f\"Sampled and shuffled {len(X_LQ_bkg)} LQ background events.\")\n",
    "\n",
    "# ────────────────── 5. Process Stop Dataset (Signal + Background) ──────────────\n",
    "print(\"\\n--- Processing Stop Dataset ---\")\n",
    "# 5.1: Load Stop signal\n",
    "X_stop_sig, y_stop_sig, w_stop_sig = load_root_data(sig_files_stop, 1)\n",
    "N_sig_stop = len(X_stop_sig)\n",
    "print(f\"Loaded {N_sig_stop} Stop signal events. Target for background is {N_sig_stop}.\")\n",
    "\n",
    "# 5.2: Load all Stop background file-by-file\n",
    "stop_bkg_dfs, stop_bkg_sizes, stop_bkg_weights = [], [], []\n",
    "for f in bkg_files_stop_flat:\n",
    "    Xf, _, wf = load_root_data([f], 0)\n",
    "    stop_bkg_dfs.append(Xf)\n",
    "    stop_bkg_sizes.append(len(Xf))\n",
    "    stop_bkg_weights.append(wf)\n",
    "total_stop_bkg_events = sum(stop_bkg_sizes)\n",
    "print(f\"Found {total_stop_bkg_events} total available Stop background events across {len(stop_bkg_dfs)} files.\")\n",
    "\n",
    "# 5.3: Determine how many events to draw from each Stop background file proportionally\n",
    "target_per_file_stop = [int(round(N_sig_stop * (sz / total_stop_bkg_events))) for sz in stop_bkg_sizes]\n",
    "while sum(target_per_file_stop) < N_sig_stop: target_per_file_stop[np.argmax(stop_bkg_sizes)] += 1\n",
    "while sum(target_per_file_stop) > N_sig_stop: target_per_file_stop[np.argmax(target_per_file_stop)] -= 1\n",
    "\n",
    "# 5.4: Sample from each Stop background file and shuffle the result\n",
    "X_stop_bkg_parts, y_stop_bkg_parts, w_stop_bkg_parts = [], [], []\n",
    "for df, wt, k in zip(stop_bkg_dfs, stop_bkg_weights, target_per_file_stop):\n",
    "    if k == 0: continue\n",
    "    sel = np.random.choice(len(df), k, replace=False)\n",
    "    X_stop_bkg_parts.append(df.iloc[sel])\n",
    "    y_stop_bkg_parts.append(np.zeros(k, dtype=np.int64))\n",
    "    w_stop_bkg_parts.append(wt[sel])\n",
    "\n",
    "X_stop_bkg = pd.concat(X_stop_bkg_parts, axis=0)\n",
    "y_stop_bkg = np.concatenate(y_stop_bkg_parts)\n",
    "w_stop_bkg = np.concatenate(w_stop_bkg_parts)\n",
    "\n",
    "stop_bkg_perm = np.random.permutation(len(X_stop_bkg))\n",
    "X_stop_bkg, y_stop_bkg, w_stop_bkg = X_stop_bkg.iloc[stop_bkg_perm], y_stop_bkg[stop_bkg_perm], w_stop_bkg[stop_bkg_perm]\n",
    "print(f\"Sampled and shuffled {len(X_stop_bkg)} Stop background events.\")\n",
    "\n",
    "\n",
    "# ────────────────── 6. Concatenate All Datasets ─────────────────────────\n",
    "print(\"\\n--- Concatenating and Final Shuffling ---\")\n",
    "X_full = pd.concat([X_LQ_sig, X_LQ_bkg, X_stop_sig, X_stop_bkg], axis=0)\n",
    "y_full = np.concatenate([y_LQ_sig, y_LQ_bkg, y_stop_sig, y_stop_bkg])\n",
    "w_full = np.concatenate([w_LQ_sig, w_LQ_bkg, w_stop_sig, w_stop_bkg])\n",
    "print(f\"Concatenated all four datasets. Total events: {len(X_full)}\")\n",
    "\n",
    "\n",
    "# ────────────────── 7. Final Double Shuffle ─────────────────────────────\n",
    "# First shuffle using numpy permutation\n",
    "print(\"Performing first shuffle (numpy)\")\n",
    "perm1 = np.random.permutation(len(X_full))\n",
    "X_full = X_full.iloc[perm1].reset_index(drop=True)\n",
    "y_full = y_full[perm1]\n",
    "w_full = w_full[perm1]\n",
    "\n",
    "# Second shuffle using scikit-learn's utility (as a different method)\n",
    "print(\"Performing second shuffle (sklearn)\")\n",
    "X_full, y_full, w_full = sk_shuffle(X_full, y_full, w_full)\n",
    "X_full = X_full.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ────────────────── 8. Final Output ─────────────────────────────────────\n",
    "print(\"\\n>>> FINAL DATASET STATS <<<\")\n",
    "print(f\"    Total events: {len(X_full)}\")\n",
    "print(f\"    Features shape: {X_full.shape}\")\n",
    "print(f\"    Final class counts (0=bkg, 1=sig): {np.bincount(y_full.astype(int))}\")\n",
    "print(f\"    (Signal should be {N_sig_LQ + N_sig_stop}, Background should be {N_sig_LQ + N_sig_stop})\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Revised azz same ol' shizz\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import h5py\n",
    "import awkward as ak\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "selected_variables = [\n",
    "    \"jet1_pt\", \"jet1_eta\", \"jet1met_dphi\", \"jet2met_dphi\",\n",
    "    \"met_sig\", \"mjj\",\n",
    "    \"pTjj\", \"mbb\", \"pTbb\", \"dRjj\", \"dEtajj\", \"dPhijj\", \"dRbb\", \"dEtabb\",\n",
    "    \"dPhibb\", \"jet2_pt\", \"jet2_eta\"\n",
    "]\n",
    "\n",
    "base_dir_LQ      = \"/home/sgoswami/monobcntuples/\"\n",
    "signal_masses_LQ = [\"500\", \"1000\", \"1400\", \"2000\", \"2500\", \"2800\"]\n",
    "\n",
    "base_dir_stop       = \"/home/sgoswami/monobcntuples/run3_btag/all\"\n",
    "signal_pattern_stop = os.path.join(base_dir_stop, \"singlestop\", \"basicSel_sT_*.root\")\n",
    "\n",
    "bkg_procs = [\"ttbar\", \"singletop\", \"dijet\", \"diboson\", \"wlnu\", \"zll\", \"znunu\"]\n",
    "\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "def importance_shuffle(X, y=None):\n",
    "    idx = np.random.permutation(len(X))\n",
    "    Xs = X.iloc[idx].reset_index(drop=True)\n",
    "    if y is None:\n",
    "        return Xs\n",
    "    ys = y[idx]\n",
    "    return Xs, ys\n",
    "\n",
    "def filter_ok(paths):\n",
    "    return [f for f in paths if \"_histogram\" not in f and \"_cutflow\" not in f]\n",
    "\n",
    "def load_root_data(file_list, label, tree_name=\"sel_tree\", max_samples=None):\n",
    "    dfs = []\n",
    "    for f in file_list:\n",
    "        with uproot.open(f) as rf:\n",
    "            if tree_name not in rf:\n",
    "                print(f\"Skipping {f}: no '{tree_name}'\")\n",
    "                continue\n",
    "            arr = rf[tree_name].arrays(selected_variables, library=\"ak\")\n",
    "        dfs.append(ak.to_dataframe(arr))\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=selected_variables), np.array([])\n",
    "    combined = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "    if max_samples:\n",
    "        replace = max_samples > len(combined)\n",
    "        combined = combined.sample(n=max_samples, replace=replace).reset_index(drop=True)\n",
    "    else:\n",
    "        combined = combined.sample(frac=1).reset_index(drop=True)\n",
    "    labels = np.full(len(combined), label)\n",
    "    return combined, labels\n",
    "\n",
    "def load_h5_data(file_list, label):\n",
    "    dfs = []\n",
    "    for f in file_list:\n",
    "        with h5py.File(f, \"r\") as hf:\n",
    "            cols0 = [n.decode() for n in hf[\"df/block0_items\"][:]]\n",
    "            vals0 = pd.DataFrame(hf[\"df/block0_values\"][:], columns=cols0)\n",
    "            cols1 = [n.decode() for n in hf[\"df/block1_items\"][:]]\n",
    "            vals1 = pd.DataFrame(hf[\"df/block1_values\"][:], columns=cols1)\n",
    "        dfs.append(pd.concat([vals0, vals1], axis=1)[selected_variables])\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=selected_variables), np.array([])\n",
    "    combined = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "    labels   = np.full(len(combined), label)\n",
    "    return combined, labels\n",
    "\n",
    "\n",
    "# --- 3. Physics-Aware Sampling Setup ---\n",
    "eff_xs = {\n",
    "    'ttbar':     729.77 * (0.10546 + 0.45623 + 0.54382),\n",
    "    'singletop': 2.0267 + 1.2675 + 36.995 + 22.173,\n",
    "    'dijet': (2.4331e9*0.00986+2.6450e7*0.01166+2.5461e5*0.01337+4.5532e3*0.01453+\n",
    "              257.54*0.00947+16.215*0.01110+0.62506*0.01015+0.01964*0.01206),\n",
    "    'diboson':   1.2974 + 4.661 + 12.079 + 0.02221 + 3.1081 + 0.57762,\n",
    "    'wlnu': (21745.0*0.14699+21814.0*0.00923+21814.0*0.14722+21814.0*0.14343+\n",
    "             21815.0*0.00912),\n",
    "    'zll': (2221.3*0.02494+2221.3*0.12896+2221.4*0.02439+2221.4*0.12920+\n",
    "            2239.7*0.02487+2239.6*0.12915+2239.6*0.84596),\n",
    "    'znunu':     448.77 * 0.20291,\n",
    "}\n",
    "_total_eff = sum(eff_xs.values())\n",
    "bkg_probs   = {p: xs/_total_eff for p, xs in eff_xs.items()}\n",
    "\n",
    "\n",
    "# --- 4. Dataset Preparation Functions ---\n",
    "def prepare_LQ():\n",
    "    sig_files = []\n",
    "    for m in signal_masses_LQ:\n",
    "        sig_files += filter_ok(glob.glob(os.path.join(base_dir_LQ, f\"mass_{m}\", f\"basicSel_mass_*.h5\")))\n",
    "\n",
    "    sig_df, sig_lbl = load_h5_data(sig_files, label=1)\n",
    "    S = len(sig_df)\n",
    "\n",
    "    bkg_counts = {p: int(round(S * bkg_probs[p])) for p in bkg_procs}\n",
    "    diff = S - sum(bkg_counts.values())\n",
    "    if diff: bkg_counts[bkg_procs[-1]] += diff\n",
    "\n",
    "    bkg_parts = []\n",
    "    for proc in bkg_procs:\n",
    "        n_req = bkg_counts[proc]\n",
    "        if n_req == 0: continue\n",
    "\n",
    "        if proc == 'wlnu':\n",
    "            dirs = ['wenu_mc20e','wmunu_mc20e','wtaunu_mc20e']\n",
    "            files = sum((glob.glob(os.path.join(base_dir_LQ,d,\"*.h5\")) for d in dirs), [])\n",
    "        elif proc == 'zll':\n",
    "            dirs = ['zee_mc20e','zmumu_mc20e','ztautau_mc20e']\n",
    "            files = sum((glob.glob(os.path.join(base_dir_LQ,d,\"*.h5\")) for d in dirs), [])\n",
    "        else:\n",
    "            files = glob.glob(os.path.join(base_dir_LQ, f\"{proc}_mc20e\", \"*.h5\"))\n",
    "\n",
    "        files = filter_ok(files)\n",
    "        if not files: continue\n",
    "\n",
    "        df_all, _ = load_h5_data(files, label=0)\n",
    "        if len(df_all) == 0: continue\n",
    "\n",
    "        replace = n_req > len(df_all)\n",
    "        df_samp = df_all.sample(n=n_req, replace=replace).reset_index(drop=True)\n",
    "        bkg_parts.append(df_samp)\n",
    "\n",
    "    bkg_df = pd.concat(bkg_parts, axis=0).reset_index(drop=True)\n",
    "    bkg_df = importance_shuffle(bkg_df)\n",
    "\n",
    "    X_LQ = pd.concat([sig_df, bkg_df], axis=0).reset_index(drop=True)\n",
    "    y_LQ = np.concatenate([sig_lbl, np.zeros(len(bkg_df),dtype=int)])\n",
    "    return importance_shuffle(X_LQ, y_LQ)\n",
    "\n",
    "def prepare_stop():\n",
    "    sig_files = filter_ok(glob.glob(signal_pattern_stop))\n",
    "    sig_df, sig_lbl = load_root_data(sig_files, label=1)\n",
    "    S = len(sig_df)\n",
    "\n",
    "    bkg_counts = {p: int(round(S * bkg_probs[p])) for p in bkg_procs}\n",
    "    diff = S - sum(bkg_counts.values())\n",
    "    if diff: bkg_counts[bkg_procs[-1]] += diff\n",
    "\n",
    "    bkg_parts = []\n",
    "    for proc in bkg_procs:\n",
    "        n_req = bkg_counts[proc]\n",
    "        if n_req == 0: continue\n",
    "\n",
    "        files = filter_ok(glob.glob(os.path.join(base_dir_stop, proc, f\"basicSel_{proc}.root\")))\n",
    "        if not files: continue\n",
    "\n",
    "        df_p, _ = load_root_data(files, label=0, max_samples=n_req)\n",
    "        bkg_parts.append(df_p)\n",
    "\n",
    "    bkg_df = pd.concat(bkg_parts, axis=0).reset_index(drop=True)\n",
    "    bkg_df = importance_shuffle(bkg_df)\n",
    "\n",
    "    X_stop = pd.concat([sig_df, bkg_df], axis=0).reset_index(drop=True)\n",
    "    y_stop = np.concatenate([sig_lbl, np.zeros(len(bkg_df),dtype=int)])\n",
    "    return importance_shuffle(X_stop, y_stop)\n",
    "\n",
    "\n",
    "# --- 5. Final Data Loading ---\n",
    "X_LQ, y_LQ = prepare_LQ()\n",
    "print(f\"Loaded LQ dataset. Shape: {X_LQ.shape}, {y_LQ.shape}\")\n",
    "X_stop, y_stop = prepare_stop()\n",
    "print(f\"Loaded Stop dataset. Shape: {X_stop.shape}, {y_stop.shape}\")\n",
    "\n",
    "X_full_df = pd.concat([X_LQ, X_stop], axis=0).reset_index(drop=True)\n",
    "y_full = np.concatenate([y_LQ, y_stop])\n",
    "X_full_df, y_full = importance_shuffle(X_full_df, y_full)\n",
    "\n",
    "print(f\"\\nCombined dataset shape: {X_full_df.shape}\")\n",
    "print(f\"Combined class distribution: {np.bincount(y_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be99d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Architecture, Optimizer, and Callbacks\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "\n",
    "FOLDS = 5\n",
    "NUM_FEATURES = 5 # Placeholder for the actual number of features\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "\n",
    "        Dense(128, kernel_initializer='he_normal', activation='gelu'),\n",
    "        #LeakyReLU(),\n",
    "        #BatchNormalization(),\n",
    "        #Dropout(0.1),\n",
    "\n",
    "        Dense(64, kernel_initializer='he_normal', activation='gelu'),\n",
    "        #LeakyReLU(),\n",
    "        #BatchNormalization(),\n",
    "        #Dropout(0.1),\n",
    "\n",
    "        Dense(32, kernel_initializer='he_normal', activation='gelu'),\n",
    "        #LeakyReLU(),\n",
    "        #BatchNormalization(),\n",
    "        #Dropout(0.1),\n",
    "\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = build_model(input_dim=NUM_FEATURES)\n",
    "\n",
    "# Compile the model with the ADAM optimizer\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        AUC(name='auc'),\n",
    "        'accuracy',\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define callbacks for training\n",
    "callbacks = [\n",
    "    # Stop training if the validation AUC doesn't improve for 15 epochs\n",
    "    EarlyStopping(monitor='val_auc', patience=15, mode='max', verbose=1),\n",
    "\n",
    "    # Reduce the learning rate if validation AUC plateaus for 5 epochs\n",
    "    ReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=5, mode='max', verbose=1, min_lr=1e-7),\n",
    "\n",
    "    # Save the best model based on validation AUC\n",
    "    ModelCheckpoint(\"model_best.keras\", monitor='val_auc', save_best_only=True, mode='max')\n",
    "]\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b62261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Revised Training Loop\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "\n",
    "# Define your desired metrics once\n",
    "metric_list = [\n",
    "    AUC(name='auc'),\n",
    "    'accuracy',\n",
    "    Precision(name='precision'),\n",
    "    Recall(name='recall')\n",
    "]\n",
    "\n",
    "\n",
    "lr = 1e-6\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "FOLDS = 5\n",
    "\n",
    "X_arr = X_full.values.astype(np.float32)\n",
    "y_arr = y_full.astype(np.int64)\n",
    "w_arr = w_full # OR np.clip(w_full.astype(np.float32), -10, 10)\n",
    "\n",
    "print(f\"Starting training with LR={lr}, BS={batch_size}\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "all_histories = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_arr, y_arr), 1):\n",
    "    print(f\"\\n=== Fold {fold}/{FOLDS} ===\")\n",
    "\n",
    "    X_tr_raw, X_va_raw = X_arr[tr_idx], X_arr[va_idx]\n",
    "    y_tr, y_va = y_arr[tr_idx], y_arr[va_idx]\n",
    "    w_tr, w_va = w_arr[tr_idx], w_arr[va_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X_tr_raw)\n",
    "    X_va = scaler.transform(X_va_raw)\n",
    "\n",
    "    print(\"Train:\", X_tr.shape, \"Val:\", X_va.shape)\n",
    "\n",
    "    model = build_model(input_dim=X_tr.shape[1])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='binary_crossentropy',\n",
    "        weighted_metrics=metric_list\n",
    "    )\n",
    "\n",
    "    # Update checkpoint to save a unique file for each fold\n",
    "    callbacks[2] = ModelCheckpoint(\n",
    "        f\"model_fold_{fold}.h5\",\n",
    "        monitor='val_auc',\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_tr,\n",
    "        y_tr,\n",
    "        sample_weight=w_tr,\n",
    "        validation_data=(X_va, y_va, w_va),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    all_histories.append(history)\n",
    "    print(\"\\n--- Fold\", fold, \"Loss Summary ---\")\n",
    "    val_loss_history = history.history['val_loss']\n",
    "    print(\"Validation loss for the first 5 epochs:\", val_loss_history[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Final Evaluation Cell (run after all folds are trained)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"--- Starting Final K-Fold Evaluation ---\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "all_true_labels = []\n",
    "all_pred_probs = []\n",
    "per_fold_aucs = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_arr, y_arr), 1):\n",
    "    print(f\"Loading and evaluating Fold {fold}...\")\n",
    "\n",
    "    X_tr_raw, X_va_raw = X_arr[tr_idx], X_arr[va_idx]\n",
    "    y_va = y_arr[va_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tr_raw)\n",
    "    X_va = scaler.transform(X_va_raw)\n",
    "\n",
    "    model = tf.keras.models.load_model(f\"model_fold_{fold}.h5\")\n",
    "\n",
    "    fold_pred_probs = model.predict(X_va, batch_size=batch_size * 2).squeeze()\n",
    "\n",
    "    all_pred_probs.append(fold_pred_probs)\n",
    "    all_true_labels.append(y_va)\n",
    "\n",
    "    fold_auc = roc_auc_score(y_va, fold_pred_probs)\n",
    "    per_fold_aucs.append(fold_auc)\n",
    "    print(f\"  AUC for Fold {fold}: {fold_auc:.4f}\")\n",
    "\n",
    "y_true_full = np.concatenate(all_true_labels)\n",
    "y_pred_full = np.concatenate(all_pred_probs)\n",
    "\n",
    "mean_auc = np.mean(per_fold_aucs)\n",
    "std_auc = np.std(per_fold_aucs)\n",
    "print(f\"\\nOverall Model Performance:\")\n",
    "print(f\"  Mean AUC = {mean_auc:.4f}\")\n",
    "print(f\"  AUC Std. Dev. = {std_auc:.4f}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true_full, y_pred_full)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f'Overall AUC = {roc_auc:.4f}', lw=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1, label='Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Overall ROC Curve (from all folds)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.hist(y_pred_full[y_true_full==0], bins=50, alpha=0.6, label='Background', density=True)\n",
    "plt.hist(y_pred_full[y_true_full==1], bins=50, alpha=0.6, label='Signal', density=True)\n",
    "plt.xlabel('Predicted Probability for Signal Class')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Overall Output Distribution (from all folds)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ROC Curve and Confusion Matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "# --- 1. Prepare Data for Plotting ---\n",
    "# Convert probabilities to class predictions for the confusion matrix\n",
    "y_pred_class = (y_pred_full > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics for the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true_full, y_pred_full)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true_full, y_pred_class)\n",
    "\n",
    "\n",
    "# --- 2. Create the Plots ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: ROC Curve\n",
    "ax1.plot(fpr, tpr, label=f'Overall AUC = {roc_auc:.4f}', lw=2)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', lw=1, label='Chance')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Overall ROC Curve')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "# Plot 2: Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['Background','Signal'])\n",
    "disp.plot(ax=ax2, cmap='Blues', colorbar=False, values_format='d')\n",
    "ax2.set_title('Overall Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4f562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
